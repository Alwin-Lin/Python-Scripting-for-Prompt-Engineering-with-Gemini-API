{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLnvVexmxATp4zuqIzfM/7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alwin-Lin/Python-Scripting-for-Prompt-Engineering-with-Gemini-API/blob/main/week1Standard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhqB5aWyOpbi"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "GEMINI_API_KEY = \"AIzaSyBBdXu6dqyYYsLffhBPvKdE7j5eU3AuQwQ\"\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    print(\"üî¥ Error: GEMINI_API_KEY not found. Please set it in your .env file.\")\n",
        "    exit()\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 1,\n",
        "    \"max_output_tokens\": 2048,\n",
        "}\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.0-flash\", # Or use \"gemini-1.5-flash\" or \"gemini-1.5-pro\" if available\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings\n",
        ")\n",
        "\n",
        "# --- Core API Call Function ---\n",
        "def call_gemini_api(prompt_text):\n",
        "    \"\"\"\n",
        "    Calls the Gemini API with the given prompt text.\n",
        "\n",
        "    Args:\n",
        "        prompt_text (str): The prompt to send to the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM's generated text response, or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\n‚ú® Sending prompt to Gemini...\")\n",
        "        print(\"------------------------------------\")\n",
        "        print(f\"üìú Prompt: \\\"{prompt_text}\\\"\")\n",
        "        print(\"------------------------------------\")\n",
        "        response = model.generate_content(prompt_text)\n",
        "        # Check for empty or blocked responses\n",
        "        if not response.parts:\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                return f\"üî¥ Error: Prompt was blocked. Reason: {response.prompt_feedback.block_reason}\"\n",
        "            else:\n",
        "                return \"üî¥ Error: Received an empty response from the API.\"\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"üî¥ An API error occurred: {e}\"\n",
        "\n",
        "# --- Prompting Techniques ---\n",
        "def zero_shot_prompting(user_text, task_description=\"Classify the sentiment of this movie review\"):\n",
        "    \"\"\"\n",
        "    Demonstrates zero-shot prompting.\n",
        "\n",
        "    Args:\n",
        "        user_text (str): The text provided by the user.\n",
        "        task_description (str): The instruction for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM's response.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Zero-Shot Prompting ---\")\n",
        "    prompt = f\"{task_description}: '{user_text}'\"\n",
        "    return call_gemini_api(prompt)\n",
        "\n",
        "def few_shot_prompting(user_text, task_description=\"Classify the sentiment of this movie review\"):\n",
        "    \"\"\"\n",
        "    Demonstrates few-shot prompting with hardcoded examples.\n",
        "\n",
        "    Args:\n",
        "        user_text (str): The text provided by the user.\n",
        "        task_description (str): The instruction for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM's response.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Few-Shot Prompting ---\")\n",
        "    examples = [\n",
        "        {\"text\": \"The movie was an absolute masterpiece, thrilling from start to finish!\", \"sentiment\": \"Positive\"},\n",
        "        {\"text\": \"I really wanted to like this film, but it was slow and predictable.\", \"sentiment\": \"Negative\"},\n",
        "        {\"text\": \"It was an okay movie, not great but not terrible either.\", \"sentiment\": \"Neutral\"}\n",
        "    ]\n",
        "\n",
        "    prompt = f\"{task_description}:\\n\\n\"\n",
        "    for ex in examples:\n",
        "        prompt += f\"Review: '{ex['text']}'\\nSentiment: {ex['sentiment']}\\n\\n\"\n",
        "    prompt += f\"Review: '{user_text}'\\nSentiment:\"\n",
        "    return call_gemini_api(prompt)\n",
        "\n",
        "def chain_of_thought_prompting(user_text, task_description=\"Classify the sentiment of this movie review and explain your reasoning\"):\n",
        "    \"\"\"\n",
        "    Demonstrates chain-of-thought prompting.\n",
        "\n",
        "    Args:\n",
        "        user_text (str): The text provided by the user.\n",
        "        task_description (str): The instruction for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM's response.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Chain-of-Thought Prompting ---\")\n",
        "    prompt = f\"{task_description}: '{user_text}'. Let's think step-by-step.\"\n",
        "    return call_gemini_api(prompt)\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Welcome to the Prompt Engineering Experimenter! üöÄ\")\n",
        "\n",
        "    user_input_text = input(\"\\nEnter the text you want to process (e.g., a movie review):\\n> \")\n",
        "\n",
        "    if not user_input_text.strip():\n",
        "        print(\"‚ö†Ô∏è No input text provided. Exiting.\")\n",
        "    else:\n",
        "        print(\"\\nüî¨ Running Experiments...\")\n",
        "\n",
        "        # Zero-shot\n",
        "        response_zero_shot = zero_shot_prompting(user_input_text)\n",
        "        print(f\"üí¨ Gemini's Zero-Shot Response:\\n{response_zero_shot}\")\n",
        "\n",
        "        # Few-shot\n",
        "        response_few_shot = few_shot_prompting(user_input_text)\n",
        "        print(f\"\\nüí¨ Gemini's Few-Shot Response:\\n{response_few_shot}\")\n",
        "\n",
        "        # Chain-of-thought\n",
        "        response_cot = chain_of_thought_prompting(user_input_text)\n",
        "        print(f\"\\nüí¨ Gemini's Chain-of-Thought Response:\\n{response_cot}\")\n",
        "\n",
        "        print(\"\\nüéâ Experiments Complete! üéâ\")\n",
        "        print(\"\\nü§î Compare and Contrast:\")\n",
        "        print(\"-------------------------\")\n",
        "        print(\"Consider the following when comparing the outputs:\")\n",
        "        print(\"1.  **Accuracy/Relevance:** How well did each technique perform the task (e.g., sentiment classification)?\")\n",
        "        print(\"2.  **Completeness of Response:** Did CoT provide a more reasoned or detailed output?\")\n",
        "        print(\"3.  **Conciseness:** Were some responses more to-the-point than others?\")\n",
        "        print(\"4.  **Ease of Implementation:** How complex was it to set up each prompt type?\")\n",
        "        print(\"5.  **Generalizability:** How might these techniques perform on different tasks or with different inputs?\")\n",
        "        print(\"\\nFor example:\")\n",
        "        print(\"-   **Zero-shot** is the simplest but might be less accurate for complex tasks if the model hasn't been explicitly trained for them.\")\n",
        "        print(\"-   **Few-shot** provides context and examples, often improving accuracy, especially for specific formats or nuanced tasks. However, selecting good examples is key.\")\n",
        "        print(\"-   **Chain-of-Thought** encourages the model to break down the problem, which can lead to better reasoning and more accurate results for complex problems, though the output might be more verbose.\")"
      ]
    }
  ]
}